{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SiamesebiLSTM_ex.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adarsa/Tensorflow/blob/master/docQuerying/SiamesebiLSTM_ex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "wDy8vI5aQONN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Siamese LSTM implementation\n",
        "\n",
        "It is keras based implementation of siamese architecture using lstm encoders to compute text similarity. Ref: https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12195\n",
        "\n",
        "This is the example usage of deep siamese Bidirectional LSTM  as given in https://github.com/amansrivastava17/lstm-siamese-text-similarity."
      ]
    },
    {
      "metadata": {
        "id": "5FBGzc09NUQg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense, Input, LSTM, Dropout, Bidirectional\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.models import load_model\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# std imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import time\n",
        "import gc\n",
        "import os\n",
        "\n",
        "def train_word2vec(documents, embedding_dim):\n",
        "    \"\"\"\n",
        "    train word2vector over traning documents\n",
        "    Args:\n",
        "        documents (list): list of document\n",
        "        min_count (int): min count of word in documents to consider for word vector creation\n",
        "        embedding_dim (int): outpu wordvector size\n",
        "    Returns:\n",
        "        word_vectors(dict): dict containing words and their respective vectors\n",
        "    \"\"\"\n",
        "    model = Word2Vec(documents, min_count=1, size=embedding_dim)\n",
        "    word_vectors = model.wv\n",
        "    del model\n",
        "    return word_vectors\n",
        "\n",
        "\n",
        "def create_embedding_matrix(tokenizer, word_vectors, embedding_dim):\n",
        "    \"\"\"\n",
        "    Create embedding matrix containing word indexes and respective vectors from word vectors\n",
        "    Args:\n",
        "        tokenizer (keras.preprocessing.text.Tokenizer): keras tokenizer object containing word indexes\n",
        "        word_vectors (dict): dict containing word and their respective vectors\n",
        "        embedding_dim (int): dimention of word vector\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "    nb_words = len(tokenizer.word_index) + 1\n",
        "    word_index = tokenizer.word_index\n",
        "    embedding_matrix = np.zeros((nb_words, embedding_dim))\n",
        "    print(\"Embedding matrix shape: %s\" % str(embedding_matrix.shape))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = word_vectors[word]\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "def word_embed_meta_data(documents, embedding_dim):\n",
        "    \"\"\"\n",
        "    Load tokenizer object for given vocabs list\n",
        "    Args:\n",
        "        documents (list): list of document\n",
        "    Returns:\n",
        "        tokenizer (keras.preprocessing.text.Tokenizer): keras tokenizer object\n",
        "        embedding_matrix (dict): dict with word_index and vector mapping\n",
        "    \"\"\"\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(\" \".join(documents))\n",
        "    word_vector = train_word2vec(documents, embedding_dim)\n",
        "    embedding_matrix = create_embedding_matrix(tokenizer, word_vector, embedding_dim)\n",
        "    del word_vector\n",
        "    #gc.collect()\n",
        "    return tokenizer, embedding_matrix\n",
        "\n",
        "\n",
        "\n",
        "def create_train_dev_set(tokenizer, sentences_pair, is_similar, max_sequence_length, validation_split_ratio):\n",
        "    \"\"\"\n",
        "    Create training and validation dataset\n",
        "    Args:\n",
        "        tokenizer (keras.preprocessing.text.Tokenizer): keras tokenizer object\n",
        "        sentences_pair (list): list of tuple of sentences pairs\n",
        "        is_similar (list): list containing labels if respective sentences in sentence1 and sentence2\n",
        "                           are same or not (1 if same else 0)\n",
        "        max_sequence_length (int): max sequence length of sentences to apply padding\n",
        "        validation_split_ratio (float): contain ratio to split training data into validation data\n",
        "    Returns:\n",
        "        train_data_1 (list): list of input features for training set from sentences1\n",
        "        train_data_2 (list): list of input features for training set from sentences2\n",
        "        labels_train (np.array): array containing similarity score for training data\n",
        "        leaks_train(np.array): array of training leaks features\n",
        "        val_data_1 (list): list of input features for validation set from sentences1\n",
        "        val_data_2 (list): list of input features for validation set from sentences1\n",
        "        labels_val (np.array): array containing similarity score for validation data\n",
        "        leaks_val (np.array): array of validation leaks features\n",
        "    \"\"\"\n",
        "    sentences1 = [x[0] for x in sentences_pair]\n",
        "    sentences2 = [x[1] for x in sentences_pair]\n",
        "    train_sequences_1 = tokenizer.texts_to_sequences(sentences1)\n",
        "    train_sequences_2 = tokenizer.texts_to_sequences(sentences2)\n",
        "    leaks = [[len(set(x1)), len(set(x2)), len(set(x1).intersection(x2))]\n",
        "             for x1, x2 in zip(train_sequences_1, train_sequences_2)]\n",
        "\n",
        "    train_padded_data_1 = pad_sequences(train_sequences_1, maxlen=max_sequence_length)\n",
        "    train_padded_data_2 = pad_sequences(train_sequences_2, maxlen=max_sequence_length)\n",
        "    train_labels = np.array(is_similar)\n",
        "    leaks = np.array(leaks)\n",
        "\n",
        "    shuffle_indices = np.random.permutation(np.arange(len(train_labels)))\n",
        "    train_data_1_shuffled = train_padded_data_1[shuffle_indices]\n",
        "    train_data_2_shuffled = train_padded_data_2[shuffle_indices]\n",
        "    train_labels_shuffled = train_labels[shuffle_indices]\n",
        "    leaks_shuffled = leaks[shuffle_indices]\n",
        "\n",
        "    dev_idx = max(1, int(len(train_labels_shuffled) * validation_split_ratio))\n",
        "\n",
        "    del train_padded_data_1\n",
        "    del train_padded_data_2\n",
        "    gc.collect()\n",
        "\n",
        "    train_data_1, val_data_1 = train_data_1_shuffled[:-dev_idx], train_data_1_shuffled[-dev_idx:]\n",
        "    train_data_2, val_data_2 = train_data_2_shuffled[:-dev_idx], train_data_2_shuffled[-dev_idx:]\n",
        "    labels_train, labels_val = train_labels_shuffled[:-dev_idx], train_labels_shuffled[-dev_idx:]\n",
        "    leaks_train, leaks_val = leaks_shuffled[:-dev_idx], leaks_shuffled[-dev_idx:]\n",
        "\n",
        "    return train_data_1, train_data_2, labels_train, leaks_train, val_data_1, val_data_2, labels_val, leaks_val\n",
        "\n",
        "\n",
        "def create_test_data(tokenizer, test_sentences_pair, max_sequence_length):\n",
        "    \"\"\"\n",
        "    Create training and validation dataset\n",
        "    Args:\n",
        "        tokenizer (keras.preprocessing.text.Tokenizer): keras tokenizer object\n",
        "        test_sentences_pair (list): list of tuple of sentences pairs\n",
        "        max_sequence_length (int): max sequence length of sentences to apply padding\n",
        "    Returns:\n",
        "        test_data_1 (list): list of input features for training set from sentences1\n",
        "        test_data_2 (list): list of input features for training set from sentences2\n",
        "    \"\"\"\n",
        "    test_sentences1 = [x[0] for x in test_sentences_pair]\n",
        "    test_sentences2 = [x[1] for x in test_sentences_pair]\n",
        "\n",
        "    test_sequences_1 = tokenizer.texts_to_sequences(test_sentences1)\n",
        "    test_sequences_2 = tokenizer.texts_to_sequences(test_sentences2)\n",
        "    leaks_test = [[len(set(x1)), len(set(x2)), len(set(x1).intersection(x2))]\n",
        "                  for x1, x2 in zip(test_sequences_1, test_sequences_2)]\n",
        "\n",
        "    leaks_test = np.array(leaks_test)\n",
        "    test_data_1 = pad_sequences(test_sequences_1, maxlen=max_sequence_length)\n",
        "    test_data_2 = pad_sequences(test_sequences_2, maxlen=max_sequence_length)\n",
        "\n",
        "    return test_data_1, test_data_2, leaks_test\n",
        "\n",
        "class SiameseBiLSTM:\n",
        "    def __init__(self, embedding_dim, max_sequence_length, number_lstm, number_dense, rate_drop_lstm, \n",
        "                 rate_drop_dense, hidden_activation, validation_split_ratio):\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.number_lstm_units = number_lstm\n",
        "        self.rate_drop_lstm = rate_drop_lstm\n",
        "        self.number_dense_units = number_dense\n",
        "        self.activation_function = hidden_activation\n",
        "        self.rate_drop_dense = rate_drop_dense\n",
        "        self.validation_split_ratio = validation_split_ratio\n",
        "\n",
        "    def train_model(self, sentences_pair, is_similar, embedding_meta_data, model_save_directory='./'):\n",
        "        \"\"\"\n",
        "        Train Siamese network to find similarity between sentences in `sentences_pair`\n",
        "            Steps Involved:\n",
        "                1. Pass the each from sentences_pairs  to bidirectional LSTM encoder.\n",
        "                2. Merge the vectors from LSTM encodes and passed to dense layer.\n",
        "                3. Pass the  dense layer vectors to sigmoid output layer.\n",
        "                4. Use cross entropy loss to train weights\n",
        "        Args:\n",
        "            sentences_pair (list): list of tuple of sentence pairs\n",
        "            is_similar (list): target value 1 if same sentences pair are similar otherwise 0\n",
        "            embedding_meta_data (dict): dict containing tokenizer and word embedding matrix\n",
        "            model_save_directory (str): working directory for where to save models\n",
        "        Returns:\n",
        "            return (best_model_path):  path of best model\n",
        "        \"\"\"\n",
        "        tokenizer, embedding_matrix = embedding_meta_data['tokenizer'], embedding_meta_data['embedding_matrix']\n",
        "\n",
        "        train_data_x1, train_data_x2, train_labels, leaks_train, \\\n",
        "        val_data_x1, val_data_x2, val_labels, leaks_val = create_train_dev_set(tokenizer, sentences_pair,\n",
        "                                                                               is_similar, self.max_sequence_length,\n",
        "                                                                               self.validation_split_ratio)\n",
        "\n",
        "        if train_data_x1 is None:\n",
        "            print(\"++++ !! Failure: Unable to train model ++++\")\n",
        "            return None\n",
        "\n",
        "        nb_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "        # Creating word embedding layer\n",
        "        embedding_layer = Embedding(nb_words, self.embedding_dim, weights=[embedding_matrix],\n",
        "                                    input_length=self.max_sequence_length, trainable=False)\n",
        "\n",
        "        # Creating LSTM Encoder\n",
        "        lstm_layer = Bidirectional(LSTM(self.number_lstm_units, dropout=self.rate_drop_lstm, recurrent_dropout=self.rate_drop_lstm))\n",
        "\n",
        "        # Creating LSTM Encoder layer for First Sentence\n",
        "        sequence_1_input = Input(shape=(self.max_sequence_length,), dtype='int32')\n",
        "        embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
        "        x1 = lstm_layer(embedded_sequences_1)\n",
        "\n",
        "        # Creating LSTM Encoder layer for Second Sentence\n",
        "        sequence_2_input = Input(shape=(self.max_sequence_length,), dtype='int32')\n",
        "        embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
        "        x2 = lstm_layer(embedded_sequences_2)\n",
        "\n",
        "        # Creating leaks input\n",
        "        leaks_input = Input(shape=(leaks_train.shape[1],))\n",
        "        leaks_dense = Dense(int(self.number_dense_units/2), activation=self.activation_function)(leaks_input)\n",
        "\n",
        "        # Merging two LSTM encodes vectors from sentences to\n",
        "        # pass it to dense layer applying dropout and batch normalisation\n",
        "        merged = concatenate([x1, x2, leaks_dense])\n",
        "        merged = BatchNormalization()(merged)\n",
        "        merged = Dropout(self.rate_drop_dense)(merged)\n",
        "        merged = Dense(self.number_dense_units, activation=self.activation_function)(merged)\n",
        "        merged = BatchNormalization()(merged)\n",
        "        merged = Dropout(self.rate_drop_dense)(merged)\n",
        "        preds = Dense(1, activation='sigmoid')(merged)\n",
        "\n",
        "        model = Model(inputs=[sequence_1_input, sequence_2_input, leaks_input], outputs=preds)\n",
        "        model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
        "\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "        STAMP = 'lstm_%d_%d_%.2f_%.2f' % (self.number_lstm_units, self.number_dense_units, self.rate_drop_lstm, self.rate_drop_dense)\n",
        "\n",
        "        checkpoint_dir = model_save_directory + 'checkpoints/' + str(int(time.time())) + '/'\n",
        "\n",
        "        if not os.path.exists(checkpoint_dir):\n",
        "            os.makedirs(checkpoint_dir)\n",
        "\n",
        "        bst_model_path = checkpoint_dir + STAMP + '.h5'\n",
        "\n",
        "        model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=False)\n",
        "\n",
        "        tensorboard = TensorBoard(log_dir=checkpoint_dir + \"logs/{}\".format(time.time()))\n",
        "\n",
        "        model.fit([train_data_x1, train_data_x2, leaks_train], train_labels,\n",
        "                  validation_data=([val_data_x1, val_data_x2, leaks_val], val_labels),\n",
        "                  epochs=200, batch_size=64, shuffle=True,\n",
        "                  callbacks=[early_stopping, model_checkpoint, tensorboard])\n",
        "\n",
        "        return bst_model_path\n",
        "\n",
        "\n",
        "    def update_model(self, saved_model_path, new_sentences_pair, is_similar, embedding_meta_data):\n",
        "        \"\"\"\n",
        "        Update trained siamese model for given new sentences pairs \n",
        "            Steps Involved:\n",
        "                1. Pass the each from sentences from new_sentences_pair to bidirectional LSTM encoder.\n",
        "                2. Merge the vectors from LSTM encodes and passed to dense layer.\n",
        "                3. Pass the  dense layer vectors to sigmoid output layer.\n",
        "                4. Use cross entropy loss to train weights\n",
        "        Args:\n",
        "            model_path (str): model path of already trained siamese model\n",
        "            new_sentences_pair (list): list of tuple of new sentences pairs\n",
        "            is_similar (list): target value 1 if same sentences pair are similar otherwise 0\n",
        "            embedding_meta_data (dict): dict containing tokenizer and word embedding matrix\n",
        "        Returns:\n",
        "            return (best_model_path):  path of best model\n",
        "        \"\"\"\n",
        "        tokenizer = embedding_meta_data['tokenizer']\n",
        "        train_data_x1, train_data_x2, train_labels, leaks_train, \\\n",
        "        val_data_x1, val_data_x2, val_labels, leaks_val = create_train_dev_set(tokenizer, new_sentences_pair,\n",
        "                                                                               is_similar, self.max_sequence_length,\n",
        "                                                                               self.validation_split_ratio)\n",
        "        model = load_model(saved_model_path)\n",
        "        model_file_name = saved_model_path.split('/')[-1]\n",
        "        new_model_checkpoint_path  = saved_model_path.split('/')[:-2] + str(int(time.time())) + '/' \n",
        "\n",
        "        new_model_path = new_model_checkpoint_path + model_file_name\n",
        "        model_checkpoint = ModelCheckpoint(new_model_checkpoint_path + model_file_name,\n",
        "                                           save_best_only=True, save_weights_only=False)\n",
        "\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "        tensorboard = TensorBoard(log_dir=new_model_checkpoint_path + \"logs/{}\".format(time.time()))\n",
        "\n",
        "        model.fit([train_data_x1, train_data_x2, leaks_train], train_labels,\n",
        "                  validation_data=([val_data_x1, val_data_x2, leaks_val], val_labels),\n",
        "                  epochs=50, batch_size=3, shuffle=True,\n",
        "                  callbacks=[early_stopping, model_checkpoint, tensorboard])\n",
        "\n",
        "        return new_model_path\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xbRpqockUdS-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Read data from google sheet**\n",
        "\n",
        "\n",
        "Add this sheet to your drive or make a copy: https://docs.google.com/spreadsheets/d/1nCk2Ko5vj14W1stSe1SpbSDJSgiQ-ScHxiGvAexhNSk/edit?usp=sharing\n"
      ]
    },
    {
      "metadata": {
        "id": "AtEV6PqpQFV1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install --upgrade -q gspread"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Timr9uVUROUc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3pfu9E8hrSMU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "\n",
        "worksheet = gc.open('Siamese_lstm').sheet1\n",
        "\n",
        "# get_all_values gives a list of rows.\n",
        "rows = worksheet.get_all_values()\n",
        "\n",
        "# Convert to a DataFrame and render.\n",
        "df=pd.DataFrame.from_records(rows,columns=rows[0])\n",
        "df=df.drop([0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6k-mI6i8tFA4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Model Configuration"
      ]
    },
    {
      "metadata": {
        "id": "c6IWT7nUsKuV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "EMBEDDING_DIM = 50 # Dimension of the dense embedding in embedding layer\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 10\n",
        "VALIDATION_SPLIT = 0.1\n",
        "\n",
        "\n",
        "RATE_DROP_LSTM = 0.17\n",
        "RATE_DROP_DENSE = 0.25\n",
        "NUMBER_LSTM = 50\n",
        "NUMBER_DENSE_UNITS = 50\n",
        "ACTIVATION_FUNCTION = 'relu'\n",
        "\n",
        "\n",
        "siamese_config = {\n",
        "\t'EMBEDDING_DIM': EMBEDDING_DIM,\n",
        "\t'MAX_SEQUENCE_LENGTH' : MAX_SEQUENCE_LENGTH,\n",
        "\t'VALIDATION_SPLIT': VALIDATION_SPLIT,\n",
        "\t'RATE_DROP_LSTM': RATE_DROP_LSTM,\n",
        "\t'RATE_DROP_DENSE': RATE_DROP_DENSE,\n",
        "\t'NUMBER_LSTM': NUMBER_LSTM,\n",
        "\t'NUMBER_DENSE_UNITS': NUMBER_DENSE_UNITS,\n",
        "\t'ACTIVATION_FUNCTION': ACTIVATION_FUNCTION\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "od7owkwxtKhI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Training**"
      ]
    },
    {
      "metadata": {
        "id": "G2nUXMebrt10",
        "colab_type": "code",
        "outputId": "ba1e678f-c742-479c-e9b0-2a79e782240b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "import gc\n",
        "sentences1 = list(df['sentences1'])\n",
        "sentences2 = list(df['sentences2'])\n",
        "is_similar = list(df['is_similar'])\n",
        "del df\n",
        "\n",
        "######## Word Embedding ############\n",
        "\n",
        "tokenizer, embedding_matrix = word_embed_meta_data(sentences1 + sentences2,  siamese_config['EMBEDDING_DIM'])\n",
        "\n",
        "embedding_meta_data = {\n",
        "\t'tokenizer': tokenizer,\n",
        "\t'embedding_matrix': embedding_matrix\n",
        "}\n",
        "\n",
        "## creating sentence pairs\n",
        "sentences_pair = [(x1, x2) for x1, x2 in zip(sentences1, sentences2)]\n",
        "del sentences1\n",
        "del sentences2\n",
        "\n",
        "######## Training ########\n",
        "\n",
        "class Configuration(object):\n",
        "    \"\"\"Dump stuff here\"\"\"\n",
        "\n",
        "CONFIG = Configuration()\n",
        "\n",
        "CONFIG.embedding_dim = siamese_config['EMBEDDING_DIM']\n",
        "CONFIG.max_sequence_length = siamese_config['MAX_SEQUENCE_LENGTH']\n",
        "CONFIG.number_lstm_units = siamese_config['NUMBER_LSTM']\n",
        "CONFIG.rate_drop_lstm = siamese_config['RATE_DROP_LSTM']\n",
        "CONFIG.number_dense_units = siamese_config['NUMBER_DENSE_UNITS']\n",
        "CONFIG.activation_function = siamese_config['ACTIVATION_FUNCTION']\n",
        "CONFIG.rate_drop_dense = siamese_config['RATE_DROP_DENSE']\n",
        "CONFIG.validation_split_ratio = siamese_config['VALIDATION_SPLIT']\n",
        "\n",
        "siamese = SiameseBiLSTM(CONFIG.embedding_dim , CONFIG.max_sequence_length, CONFIG.number_lstm_units , CONFIG.number_dense_units, CONFIG.rate_drop_lstm, CONFIG.rate_drop_dense, CONFIG.activation_function, CONFIG.validation_split_ratio)\n",
        "\n",
        "best_model_path = siamese.train_model(sentences_pair, is_similar, embedding_meta_data, model_save_directory='./')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding matrix shape: (49, 50)\n",
            "Null word embeddings: 1\n",
            "Train on 450 samples, validate on 49 samples\n",
            "Epoch 1/200\n",
            "450/450 [==============================] - 5s 11ms/step - loss: 0.8123 - acc: 0.4933 - val_loss: 0.7030 - val_acc: 0.5510\n",
            "Epoch 2/200\n",
            "450/450 [==============================] - 0s 683us/step - loss: 0.7402 - acc: 0.5444 - val_loss: 0.7888 - val_acc: 0.5306\n",
            "Epoch 3/200\n",
            "450/450 [==============================] - 0s 751us/step - loss: 0.7372 - acc: 0.5489 - val_loss: 0.7161 - val_acc: 0.5102\n",
            "Epoch 4/200\n",
            "450/450 [==============================] - 0s 749us/step - loss: 0.7823 - acc: 0.5156 - val_loss: 0.6858 - val_acc: 0.5306\n",
            "Epoch 5/200\n",
            "450/450 [==============================] - 0s 763us/step - loss: 0.7033 - acc: 0.5733 - val_loss: 0.7399 - val_acc: 0.5510\n",
            "Epoch 6/200\n",
            "450/450 [==============================] - 0s 748us/step - loss: 0.6943 - acc: 0.5533 - val_loss: 0.6870 - val_acc: 0.5510\n",
            "Epoch 7/200\n",
            "450/450 [==============================] - 0s 747us/step - loss: 0.7039 - acc: 0.5556 - val_loss: 0.6940 - val_acc: 0.5714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jmsxOHoQvJ3z",
        "colab_type": "code",
        "outputId": "3271414d-cb2e-469b-c65c-7c24b334ade2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1617
        }
      },
      "cell_type": "code",
      "source": [
        "pd.DataFrame(embedding_matrix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.152393</td>\n",
              "      <td>-0.416267</td>\n",
              "      <td>0.030145</td>\n",
              "      <td>-0.184082</td>\n",
              "      <td>-0.010394</td>\n",
              "      <td>0.014373</td>\n",
              "      <td>-0.381159</td>\n",
              "      <td>0.049737</td>\n",
              "      <td>-0.049526</td>\n",
              "      <td>-0.164221</td>\n",
              "      <td>...</td>\n",
              "      <td>0.166599</td>\n",
              "      <td>-0.032823</td>\n",
              "      <td>0.178357</td>\n",
              "      <td>-0.335843</td>\n",
              "      <td>0.146869</td>\n",
              "      <td>-0.470962</td>\n",
              "      <td>0.395400</td>\n",
              "      <td>-0.283806</td>\n",
              "      <td>0.012441</td>\n",
              "      <td>-0.268792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.159556</td>\n",
              "      <td>-0.416011</td>\n",
              "      <td>0.045277</td>\n",
              "      <td>-0.178962</td>\n",
              "      <td>-0.038984</td>\n",
              "      <td>0.019003</td>\n",
              "      <td>-0.383190</td>\n",
              "      <td>0.090034</td>\n",
              "      <td>-0.052516</td>\n",
              "      <td>-0.163107</td>\n",
              "      <td>...</td>\n",
              "      <td>0.164741</td>\n",
              "      <td>-0.029205</td>\n",
              "      <td>0.210783</td>\n",
              "      <td>-0.320233</td>\n",
              "      <td>0.155152</td>\n",
              "      <td>-0.453669</td>\n",
              "      <td>0.422672</td>\n",
              "      <td>-0.289768</td>\n",
              "      <td>-0.012544</td>\n",
              "      <td>-0.287482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.165171</td>\n",
              "      <td>-0.473175</td>\n",
              "      <td>0.043206</td>\n",
              "      <td>-0.210630</td>\n",
              "      <td>-0.084367</td>\n",
              "      <td>0.019195</td>\n",
              "      <td>-0.367736</td>\n",
              "      <td>0.125834</td>\n",
              "      <td>-0.021669</td>\n",
              "      <td>-0.154705</td>\n",
              "      <td>...</td>\n",
              "      <td>0.160588</td>\n",
              "      <td>-0.022733</td>\n",
              "      <td>0.267268</td>\n",
              "      <td>-0.264041</td>\n",
              "      <td>0.155710</td>\n",
              "      <td>-0.411877</td>\n",
              "      <td>0.391534</td>\n",
              "      <td>-0.261104</td>\n",
              "      <td>-0.058292</td>\n",
              "      <td>-0.228538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.116846</td>\n",
              "      <td>-0.401664</td>\n",
              "      <td>0.018386</td>\n",
              "      <td>-0.184641</td>\n",
              "      <td>-0.030639</td>\n",
              "      <td>0.003306</td>\n",
              "      <td>-0.373258</td>\n",
              "      <td>0.049373</td>\n",
              "      <td>-0.072604</td>\n",
              "      <td>-0.159970</td>\n",
              "      <td>...</td>\n",
              "      <td>0.176916</td>\n",
              "      <td>-0.018187</td>\n",
              "      <td>0.153957</td>\n",
              "      <td>-0.338802</td>\n",
              "      <td>0.137904</td>\n",
              "      <td>-0.475165</td>\n",
              "      <td>0.399816</td>\n",
              "      <td>-0.319382</td>\n",
              "      <td>0.042911</td>\n",
              "      <td>-0.280524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.141465</td>\n",
              "      <td>-0.384300</td>\n",
              "      <td>0.016896</td>\n",
              "      <td>-0.162110</td>\n",
              "      <td>-0.051683</td>\n",
              "      <td>0.004966</td>\n",
              "      <td>-0.369213</td>\n",
              "      <td>0.071801</td>\n",
              "      <td>-0.044377</td>\n",
              "      <td>-0.149465</td>\n",
              "      <td>...</td>\n",
              "      <td>0.156825</td>\n",
              "      <td>-0.028656</td>\n",
              "      <td>0.198274</td>\n",
              "      <td>-0.304889</td>\n",
              "      <td>0.146665</td>\n",
              "      <td>-0.450109</td>\n",
              "      <td>0.394797</td>\n",
              "      <td>-0.290799</td>\n",
              "      <td>-0.008019</td>\n",
              "      <td>-0.269526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.166352</td>\n",
              "      <td>-0.469436</td>\n",
              "      <td>0.008980</td>\n",
              "      <td>-0.197826</td>\n",
              "      <td>-0.099537</td>\n",
              "      <td>0.031281</td>\n",
              "      <td>-0.373684</td>\n",
              "      <td>0.109303</td>\n",
              "      <td>-0.043478</td>\n",
              "      <td>-0.165713</td>\n",
              "      <td>...</td>\n",
              "      <td>0.153302</td>\n",
              "      <td>-0.031705</td>\n",
              "      <td>0.270461</td>\n",
              "      <td>-0.290961</td>\n",
              "      <td>0.125270</td>\n",
              "      <td>-0.424231</td>\n",
              "      <td>0.403370</td>\n",
              "      <td>-0.244946</td>\n",
              "      <td>-0.033692</td>\n",
              "      <td>-0.220371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.149645</td>\n",
              "      <td>-0.411599</td>\n",
              "      <td>0.010558</td>\n",
              "      <td>-0.186049</td>\n",
              "      <td>-0.039491</td>\n",
              "      <td>0.012580</td>\n",
              "      <td>-0.374992</td>\n",
              "      <td>0.079210</td>\n",
              "      <td>-0.049920</td>\n",
              "      <td>-0.154188</td>\n",
              "      <td>...</td>\n",
              "      <td>0.150584</td>\n",
              "      <td>-0.023231</td>\n",
              "      <td>0.220580</td>\n",
              "      <td>-0.321817</td>\n",
              "      <td>0.139477</td>\n",
              "      <td>-0.430551</td>\n",
              "      <td>0.406725</td>\n",
              "      <td>-0.280014</td>\n",
              "      <td>0.007822</td>\n",
              "      <td>-0.259643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.140083</td>\n",
              "      <td>-0.413539</td>\n",
              "      <td>0.013998</td>\n",
              "      <td>-0.192300</td>\n",
              "      <td>-0.058990</td>\n",
              "      <td>0.018132</td>\n",
              "      <td>-0.367364</td>\n",
              "      <td>0.084785</td>\n",
              "      <td>-0.042703</td>\n",
              "      <td>-0.153140</td>\n",
              "      <td>...</td>\n",
              "      <td>0.143403</td>\n",
              "      <td>-0.028404</td>\n",
              "      <td>0.219374</td>\n",
              "      <td>-0.318501</td>\n",
              "      <td>0.148712</td>\n",
              "      <td>-0.445012</td>\n",
              "      <td>0.404945</td>\n",
              "      <td>-0.276043</td>\n",
              "      <td>-0.022083</td>\n",
              "      <td>-0.253724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.154283</td>\n",
              "      <td>-0.358461</td>\n",
              "      <td>0.027813</td>\n",
              "      <td>-0.162773</td>\n",
              "      <td>-0.021588</td>\n",
              "      <td>0.004086</td>\n",
              "      <td>-0.379868</td>\n",
              "      <td>0.057855</td>\n",
              "      <td>-0.037082</td>\n",
              "      <td>-0.157236</td>\n",
              "      <td>...</td>\n",
              "      <td>0.129902</td>\n",
              "      <td>-0.025992</td>\n",
              "      <td>0.171931</td>\n",
              "      <td>-0.332101</td>\n",
              "      <td>0.152847</td>\n",
              "      <td>-0.401829</td>\n",
              "      <td>0.405083</td>\n",
              "      <td>-0.291005</td>\n",
              "      <td>-0.011911</td>\n",
              "      <td>-0.250736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.161227</td>\n",
              "      <td>-0.442374</td>\n",
              "      <td>0.014481</td>\n",
              "      <td>-0.199751</td>\n",
              "      <td>-0.089896</td>\n",
              "      <td>0.034551</td>\n",
              "      <td>-0.346453</td>\n",
              "      <td>0.123110</td>\n",
              "      <td>-0.027605</td>\n",
              "      <td>-0.152852</td>\n",
              "      <td>...</td>\n",
              "      <td>0.137941</td>\n",
              "      <td>-0.030926</td>\n",
              "      <td>0.261681</td>\n",
              "      <td>-0.257220</td>\n",
              "      <td>0.110564</td>\n",
              "      <td>-0.374768</td>\n",
              "      <td>0.370192</td>\n",
              "      <td>-0.204983</td>\n",
              "      <td>-0.050538</td>\n",
              "      <td>-0.204457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.155886</td>\n",
              "      <td>-0.447800</td>\n",
              "      <td>0.018727</td>\n",
              "      <td>-0.185921</td>\n",
              "      <td>-0.073807</td>\n",
              "      <td>0.018182</td>\n",
              "      <td>-0.368839</td>\n",
              "      <td>0.105111</td>\n",
              "      <td>-0.028799</td>\n",
              "      <td>-0.155093</td>\n",
              "      <td>...</td>\n",
              "      <td>0.145346</td>\n",
              "      <td>-0.041099</td>\n",
              "      <td>0.232017</td>\n",
              "      <td>-0.299460</td>\n",
              "      <td>0.148102</td>\n",
              "      <td>-0.425937</td>\n",
              "      <td>0.400460</td>\n",
              "      <td>-0.256242</td>\n",
              "      <td>-0.026433</td>\n",
              "      <td>-0.248019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.162361</td>\n",
              "      <td>-0.455236</td>\n",
              "      <td>0.052984</td>\n",
              "      <td>-0.212550</td>\n",
              "      <td>-0.099726</td>\n",
              "      <td>0.018839</td>\n",
              "      <td>-0.332478</td>\n",
              "      <td>0.139945</td>\n",
              "      <td>-0.005306</td>\n",
              "      <td>-0.179349</td>\n",
              "      <td>...</td>\n",
              "      <td>0.159146</td>\n",
              "      <td>-0.026906</td>\n",
              "      <td>0.261907</td>\n",
              "      <td>-0.228763</td>\n",
              "      <td>0.123500</td>\n",
              "      <td>-0.343418</td>\n",
              "      <td>0.365565</td>\n",
              "      <td>-0.178925</td>\n",
              "      <td>-0.054551</td>\n",
              "      <td>-0.190233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.143696</td>\n",
              "      <td>-0.427579</td>\n",
              "      <td>0.014258</td>\n",
              "      <td>-0.193974</td>\n",
              "      <td>-0.058986</td>\n",
              "      <td>0.019184</td>\n",
              "      <td>-0.354232</td>\n",
              "      <td>0.097019</td>\n",
              "      <td>-0.044185</td>\n",
              "      <td>-0.166540</td>\n",
              "      <td>...</td>\n",
              "      <td>0.163962</td>\n",
              "      <td>-0.037522</td>\n",
              "      <td>0.236092</td>\n",
              "      <td>-0.288357</td>\n",
              "      <td>0.128580</td>\n",
              "      <td>-0.407287</td>\n",
              "      <td>0.369781</td>\n",
              "      <td>-0.233996</td>\n",
              "      <td>-0.007775</td>\n",
              "      <td>-0.241931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.145564</td>\n",
              "      <td>-0.404659</td>\n",
              "      <td>0.012986</td>\n",
              "      <td>-0.178703</td>\n",
              "      <td>-0.071463</td>\n",
              "      <td>0.010354</td>\n",
              "      <td>-0.335715</td>\n",
              "      <td>0.102695</td>\n",
              "      <td>-0.031890</td>\n",
              "      <td>-0.158180</td>\n",
              "      <td>...</td>\n",
              "      <td>0.136381</td>\n",
              "      <td>-0.033189</td>\n",
              "      <td>0.225300</td>\n",
              "      <td>-0.259739</td>\n",
              "      <td>0.122836</td>\n",
              "      <td>-0.388248</td>\n",
              "      <td>0.365997</td>\n",
              "      <td>-0.225620</td>\n",
              "      <td>-0.019534</td>\n",
              "      <td>-0.215720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.168089</td>\n",
              "      <td>-0.457519</td>\n",
              "      <td>0.013693</td>\n",
              "      <td>-0.196008</td>\n",
              "      <td>-0.086993</td>\n",
              "      <td>0.023595</td>\n",
              "      <td>-0.365521</td>\n",
              "      <td>0.106462</td>\n",
              "      <td>-0.042469</td>\n",
              "      <td>-0.173147</td>\n",
              "      <td>...</td>\n",
              "      <td>0.155501</td>\n",
              "      <td>-0.035328</td>\n",
              "      <td>0.257853</td>\n",
              "      <td>-0.284918</td>\n",
              "      <td>0.117309</td>\n",
              "      <td>-0.406414</td>\n",
              "      <td>0.393192</td>\n",
              "      <td>-0.230904</td>\n",
              "      <td>-0.051237</td>\n",
              "      <td>-0.221984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.169317</td>\n",
              "      <td>-0.424889</td>\n",
              "      <td>0.028299</td>\n",
              "      <td>-0.191357</td>\n",
              "      <td>-0.067590</td>\n",
              "      <td>0.025758</td>\n",
              "      <td>-0.337540</td>\n",
              "      <td>0.098285</td>\n",
              "      <td>-0.020237</td>\n",
              "      <td>-0.150555</td>\n",
              "      <td>...</td>\n",
              "      <td>0.150408</td>\n",
              "      <td>-0.030167</td>\n",
              "      <td>0.221441</td>\n",
              "      <td>-0.240097</td>\n",
              "      <td>0.123303</td>\n",
              "      <td>-0.352163</td>\n",
              "      <td>0.344231</td>\n",
              "      <td>-0.208145</td>\n",
              "      <td>-0.038033</td>\n",
              "      <td>-0.203953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.161842</td>\n",
              "      <td>-0.418257</td>\n",
              "      <td>0.007604</td>\n",
              "      <td>-0.175646</td>\n",
              "      <td>-0.058127</td>\n",
              "      <td>0.031463</td>\n",
              "      <td>-0.340421</td>\n",
              "      <td>0.098849</td>\n",
              "      <td>-0.039084</td>\n",
              "      <td>-0.142567</td>\n",
              "      <td>...</td>\n",
              "      <td>0.133845</td>\n",
              "      <td>-0.019111</td>\n",
              "      <td>0.233122</td>\n",
              "      <td>-0.264521</td>\n",
              "      <td>0.102578</td>\n",
              "      <td>-0.375068</td>\n",
              "      <td>0.357668</td>\n",
              "      <td>-0.205485</td>\n",
              "      <td>-0.025895</td>\n",
              "      <td>-0.204281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.151130</td>\n",
              "      <td>-0.409978</td>\n",
              "      <td>-0.001362</td>\n",
              "      <td>-0.177443</td>\n",
              "      <td>-0.056593</td>\n",
              "      <td>0.019374</td>\n",
              "      <td>-0.337033</td>\n",
              "      <td>0.083588</td>\n",
              "      <td>-0.039911</td>\n",
              "      <td>-0.134523</td>\n",
              "      <td>...</td>\n",
              "      <td>0.156037</td>\n",
              "      <td>-0.023175</td>\n",
              "      <td>0.231816</td>\n",
              "      <td>-0.275182</td>\n",
              "      <td>0.110859</td>\n",
              "      <td>-0.413552</td>\n",
              "      <td>0.356258</td>\n",
              "      <td>-0.242073</td>\n",
              "      <td>-0.017939</td>\n",
              "      <td>-0.216695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.140722</td>\n",
              "      <td>-0.410099</td>\n",
              "      <td>-0.006606</td>\n",
              "      <td>-0.186926</td>\n",
              "      <td>-0.057433</td>\n",
              "      <td>0.009434</td>\n",
              "      <td>-0.347329</td>\n",
              "      <td>0.084337</td>\n",
              "      <td>-0.035972</td>\n",
              "      <td>-0.141067</td>\n",
              "      <td>...</td>\n",
              "      <td>0.139103</td>\n",
              "      <td>-0.039580</td>\n",
              "      <td>0.219179</td>\n",
              "      <td>-0.265479</td>\n",
              "      <td>0.105687</td>\n",
              "      <td>-0.380631</td>\n",
              "      <td>0.337854</td>\n",
              "      <td>-0.223201</td>\n",
              "      <td>-0.020439</td>\n",
              "      <td>-0.199714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.129275</td>\n",
              "      <td>-0.371645</td>\n",
              "      <td>0.001673</td>\n",
              "      <td>-0.171600</td>\n",
              "      <td>-0.063296</td>\n",
              "      <td>0.023654</td>\n",
              "      <td>-0.326359</td>\n",
              "      <td>0.089162</td>\n",
              "      <td>-0.043492</td>\n",
              "      <td>-0.131944</td>\n",
              "      <td>...</td>\n",
              "      <td>0.149934</td>\n",
              "      <td>-0.025618</td>\n",
              "      <td>0.203660</td>\n",
              "      <td>-0.272245</td>\n",
              "      <td>0.124562</td>\n",
              "      <td>-0.400280</td>\n",
              "      <td>0.370291</td>\n",
              "      <td>-0.238899</td>\n",
              "      <td>-0.004114</td>\n",
              "      <td>-0.228013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.178906</td>\n",
              "      <td>-0.447389</td>\n",
              "      <td>0.018909</td>\n",
              "      <td>-0.204372</td>\n",
              "      <td>-0.077419</td>\n",
              "      <td>0.027247</td>\n",
              "      <td>-0.363948</td>\n",
              "      <td>0.112033</td>\n",
              "      <td>-0.030628</td>\n",
              "      <td>-0.162722</td>\n",
              "      <td>...</td>\n",
              "      <td>0.154639</td>\n",
              "      <td>-0.036913</td>\n",
              "      <td>0.257221</td>\n",
              "      <td>-0.265650</td>\n",
              "      <td>0.120205</td>\n",
              "      <td>-0.379246</td>\n",
              "      <td>0.376951</td>\n",
              "      <td>-0.212463</td>\n",
              "      <td>-0.037797</td>\n",
              "      <td>-0.212692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.136288</td>\n",
              "      <td>-0.395735</td>\n",
              "      <td>0.020039</td>\n",
              "      <td>-0.181087</td>\n",
              "      <td>-0.075776</td>\n",
              "      <td>0.014532</td>\n",
              "      <td>-0.321146</td>\n",
              "      <td>0.093421</td>\n",
              "      <td>-0.038821</td>\n",
              "      <td>-0.147707</td>\n",
              "      <td>...</td>\n",
              "      <td>0.143629</td>\n",
              "      <td>-0.013621</td>\n",
              "      <td>0.198745</td>\n",
              "      <td>-0.251379</td>\n",
              "      <td>0.116586</td>\n",
              "      <td>-0.379648</td>\n",
              "      <td>0.348729</td>\n",
              "      <td>-0.225348</td>\n",
              "      <td>-0.004521</td>\n",
              "      <td>-0.212112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.164449</td>\n",
              "      <td>-0.452591</td>\n",
              "      <td>0.008049</td>\n",
              "      <td>-0.198805</td>\n",
              "      <td>-0.076200</td>\n",
              "      <td>0.022188</td>\n",
              "      <td>-0.387635</td>\n",
              "      <td>0.116562</td>\n",
              "      <td>-0.015153</td>\n",
              "      <td>-0.150241</td>\n",
              "      <td>...</td>\n",
              "      <td>0.164896</td>\n",
              "      <td>-0.020709</td>\n",
              "      <td>0.256107</td>\n",
              "      <td>-0.277784</td>\n",
              "      <td>0.125921</td>\n",
              "      <td>-0.403735</td>\n",
              "      <td>0.373134</td>\n",
              "      <td>-0.235530</td>\n",
              "      <td>-0.052409</td>\n",
              "      <td>-0.226917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.178088</td>\n",
              "      <td>-0.463304</td>\n",
              "      <td>0.005490</td>\n",
              "      <td>-0.215836</td>\n",
              "      <td>-0.082049</td>\n",
              "      <td>0.022671</td>\n",
              "      <td>-0.400235</td>\n",
              "      <td>0.124827</td>\n",
              "      <td>-0.020150</td>\n",
              "      <td>-0.160930</td>\n",
              "      <td>...</td>\n",
              "      <td>0.152545</td>\n",
              "      <td>-0.020881</td>\n",
              "      <td>0.274300</td>\n",
              "      <td>-0.281758</td>\n",
              "      <td>0.129352</td>\n",
              "      <td>-0.417016</td>\n",
              "      <td>0.393995</td>\n",
              "      <td>-0.249052</td>\n",
              "      <td>-0.050764</td>\n",
              "      <td>-0.223596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.362555</td>\n",
              "      <td>-0.718820</td>\n",
              "      <td>-0.102157</td>\n",
              "      <td>-0.309125</td>\n",
              "      <td>-0.318396</td>\n",
              "      <td>0.144660</td>\n",
              "      <td>-0.558377</td>\n",
              "      <td>0.332495</td>\n",
              "      <td>0.034540</td>\n",
              "      <td>-0.147916</td>\n",
              "      <td>...</td>\n",
              "      <td>0.239089</td>\n",
              "      <td>0.043642</td>\n",
              "      <td>0.643740</td>\n",
              "      <td>-0.206322</td>\n",
              "      <td>0.013518</td>\n",
              "      <td>-0.510323</td>\n",
              "      <td>0.507142</td>\n",
              "      <td>0.036080</td>\n",
              "      <td>-0.277276</td>\n",
              "      <td>-0.131295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.146703</td>\n",
              "      <td>-0.387724</td>\n",
              "      <td>-0.000509</td>\n",
              "      <td>-0.178179</td>\n",
              "      <td>-0.065141</td>\n",
              "      <td>0.035494</td>\n",
              "      <td>-0.306295</td>\n",
              "      <td>0.096835</td>\n",
              "      <td>-0.029075</td>\n",
              "      <td>-0.125234</td>\n",
              "      <td>...</td>\n",
              "      <td>0.134478</td>\n",
              "      <td>-0.013227</td>\n",
              "      <td>0.231356</td>\n",
              "      <td>-0.223501</td>\n",
              "      <td>0.101697</td>\n",
              "      <td>-0.340727</td>\n",
              "      <td>0.311853</td>\n",
              "      <td>-0.189669</td>\n",
              "      <td>-0.040087</td>\n",
              "      <td>-0.190094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.297862</td>\n",
              "      <td>-0.697382</td>\n",
              "      <td>-0.025332</td>\n",
              "      <td>-0.314447</td>\n",
              "      <td>-0.204552</td>\n",
              "      <td>0.075596</td>\n",
              "      <td>-0.555558</td>\n",
              "      <td>0.256317</td>\n",
              "      <td>0.010092</td>\n",
              "      <td>-0.197969</td>\n",
              "      <td>...</td>\n",
              "      <td>0.232850</td>\n",
              "      <td>-0.008428</td>\n",
              "      <td>0.484415</td>\n",
              "      <td>-0.314565</td>\n",
              "      <td>0.103996</td>\n",
              "      <td>-0.544687</td>\n",
              "      <td>0.531808</td>\n",
              "      <td>-0.182946</td>\n",
              "      <td>-0.161261</td>\n",
              "      <td>-0.225704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.154398</td>\n",
              "      <td>-0.415433</td>\n",
              "      <td>0.010304</td>\n",
              "      <td>-0.184295</td>\n",
              "      <td>-0.076809</td>\n",
              "      <td>0.026254</td>\n",
              "      <td>-0.354758</td>\n",
              "      <td>0.099068</td>\n",
              "      <td>-0.021618</td>\n",
              "      <td>-0.146321</td>\n",
              "      <td>...</td>\n",
              "      <td>0.137758</td>\n",
              "      <td>-0.026532</td>\n",
              "      <td>0.245911</td>\n",
              "      <td>-0.249538</td>\n",
              "      <td>0.111585</td>\n",
              "      <td>-0.382625</td>\n",
              "      <td>0.356162</td>\n",
              "      <td>-0.223341</td>\n",
              "      <td>-0.039649</td>\n",
              "      <td>-0.191929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.299048</td>\n",
              "      <td>-0.703128</td>\n",
              "      <td>-0.005496</td>\n",
              "      <td>-0.307518</td>\n",
              "      <td>-0.168284</td>\n",
              "      <td>0.071806</td>\n",
              "      <td>-0.559675</td>\n",
              "      <td>0.214469</td>\n",
              "      <td>-0.002554</td>\n",
              "      <td>-0.214621</td>\n",
              "      <td>...</td>\n",
              "      <td>0.231295</td>\n",
              "      <td>-0.027927</td>\n",
              "      <td>0.446648</td>\n",
              "      <td>-0.348223</td>\n",
              "      <td>0.140533</td>\n",
              "      <td>-0.551988</td>\n",
              "      <td>0.544470</td>\n",
              "      <td>-0.253276</td>\n",
              "      <td>-0.109701</td>\n",
              "      <td>-0.272835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.161930</td>\n",
              "      <td>-0.404691</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>-0.184292</td>\n",
              "      <td>-0.087165</td>\n",
              "      <td>0.042113</td>\n",
              "      <td>-0.339819</td>\n",
              "      <td>0.111491</td>\n",
              "      <td>-0.017846</td>\n",
              "      <td>-0.130760</td>\n",
              "      <td>...</td>\n",
              "      <td>0.136056</td>\n",
              "      <td>-0.014352</td>\n",
              "      <td>0.256241</td>\n",
              "      <td>-0.223289</td>\n",
              "      <td>0.110703</td>\n",
              "      <td>-0.349291</td>\n",
              "      <td>0.342871</td>\n",
              "      <td>-0.200392</td>\n",
              "      <td>-0.058272</td>\n",
              "      <td>-0.191546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.262786</td>\n",
              "      <td>-0.660672</td>\n",
              "      <td>0.014405</td>\n",
              "      <td>-0.295140</td>\n",
              "      <td>-0.150632</td>\n",
              "      <td>0.046025</td>\n",
              "      <td>-0.533821</td>\n",
              "      <td>0.197730</td>\n",
              "      <td>-0.011116</td>\n",
              "      <td>-0.218248</td>\n",
              "      <td>...</td>\n",
              "      <td>0.218821</td>\n",
              "      <td>-0.029363</td>\n",
              "      <td>0.402148</td>\n",
              "      <td>-0.358907</td>\n",
              "      <td>0.140617</td>\n",
              "      <td>-0.536718</td>\n",
              "      <td>0.520189</td>\n",
              "      <td>-0.272818</td>\n",
              "      <td>-0.101143</td>\n",
              "      <td>-0.275408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.270357</td>\n",
              "      <td>-0.620131</td>\n",
              "      <td>-0.013387</td>\n",
              "      <td>-0.269225</td>\n",
              "      <td>-0.165449</td>\n",
              "      <td>0.065782</td>\n",
              "      <td>-0.520012</td>\n",
              "      <td>0.206397</td>\n",
              "      <td>-0.002897</td>\n",
              "      <td>-0.192744</td>\n",
              "      <td>...</td>\n",
              "      <td>0.205340</td>\n",
              "      <td>-0.015874</td>\n",
              "      <td>0.416355</td>\n",
              "      <td>-0.326325</td>\n",
              "      <td>0.120232</td>\n",
              "      <td>-0.507643</td>\n",
              "      <td>0.492229</td>\n",
              "      <td>-0.226345</td>\n",
              "      <td>-0.120845</td>\n",
              "      <td>-0.250378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.240564</td>\n",
              "      <td>-0.578682</td>\n",
              "      <td>0.004710</td>\n",
              "      <td>-0.253639</td>\n",
              "      <td>-0.144323</td>\n",
              "      <td>0.050925</td>\n",
              "      <td>-0.473558</td>\n",
              "      <td>0.179806</td>\n",
              "      <td>-0.005000</td>\n",
              "      <td>-0.181215</td>\n",
              "      <td>...</td>\n",
              "      <td>0.191583</td>\n",
              "      <td>-0.017583</td>\n",
              "      <td>0.359153</td>\n",
              "      <td>-0.308230</td>\n",
              "      <td>0.121115</td>\n",
              "      <td>-0.475700</td>\n",
              "      <td>0.463977</td>\n",
              "      <td>-0.220983</td>\n",
              "      <td>-0.092057</td>\n",
              "      <td>-0.247100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.333930</td>\n",
              "      <td>-0.801938</td>\n",
              "      <td>0.019306</td>\n",
              "      <td>-0.343139</td>\n",
              "      <td>-0.189044</td>\n",
              "      <td>0.064471</td>\n",
              "      <td>-0.651507</td>\n",
              "      <td>0.239165</td>\n",
              "      <td>-0.031385</td>\n",
              "      <td>-0.267168</td>\n",
              "      <td>...</td>\n",
              "      <td>0.268294</td>\n",
              "      <td>-0.029644</td>\n",
              "      <td>0.500985</td>\n",
              "      <td>-0.436792</td>\n",
              "      <td>0.197332</td>\n",
              "      <td>-0.675428</td>\n",
              "      <td>0.659791</td>\n",
              "      <td>-0.359941</td>\n",
              "      <td>-0.121163</td>\n",
              "      <td>-0.351272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.220599</td>\n",
              "      <td>-0.524615</td>\n",
              "      <td>0.002625</td>\n",
              "      <td>-0.234627</td>\n",
              "      <td>-0.115668</td>\n",
              "      <td>0.037460</td>\n",
              "      <td>-0.439696</td>\n",
              "      <td>0.152936</td>\n",
              "      <td>-0.014268</td>\n",
              "      <td>-0.167562</td>\n",
              "      <td>...</td>\n",
              "      <td>0.174674</td>\n",
              "      <td>-0.018453</td>\n",
              "      <td>0.319271</td>\n",
              "      <td>-0.280031</td>\n",
              "      <td>0.128991</td>\n",
              "      <td>-0.432398</td>\n",
              "      <td>0.419654</td>\n",
              "      <td>-0.222269</td>\n",
              "      <td>-0.068565</td>\n",
              "      <td>-0.228950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.162020</td>\n",
              "      <td>-0.403671</td>\n",
              "      <td>-0.004301</td>\n",
              "      <td>-0.182414</td>\n",
              "      <td>-0.084728</td>\n",
              "      <td>0.035025</td>\n",
              "      <td>-0.336245</td>\n",
              "      <td>0.116388</td>\n",
              "      <td>-0.007195</td>\n",
              "      <td>-0.141909</td>\n",
              "      <td>...</td>\n",
              "      <td>0.127347</td>\n",
              "      <td>-0.020683</td>\n",
              "      <td>0.257507</td>\n",
              "      <td>-0.212490</td>\n",
              "      <td>0.087714</td>\n",
              "      <td>-0.334785</td>\n",
              "      <td>0.328358</td>\n",
              "      <td>-0.171171</td>\n",
              "      <td>-0.061216</td>\n",
              "      <td>-0.168949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.161339</td>\n",
              "      <td>-0.417299</td>\n",
              "      <td>0.007067</td>\n",
              "      <td>-0.189233</td>\n",
              "      <td>-0.087707</td>\n",
              "      <td>0.036001</td>\n",
              "      <td>-0.345543</td>\n",
              "      <td>0.121980</td>\n",
              "      <td>-0.013940</td>\n",
              "      <td>-0.143837</td>\n",
              "      <td>...</td>\n",
              "      <td>0.140387</td>\n",
              "      <td>-0.028504</td>\n",
              "      <td>0.262242</td>\n",
              "      <td>-0.222087</td>\n",
              "      <td>0.091713</td>\n",
              "      <td>-0.345795</td>\n",
              "      <td>0.331689</td>\n",
              "      <td>-0.181318</td>\n",
              "      <td>-0.058511</td>\n",
              "      <td>-0.182921</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.040550</td>\n",
              "      <td>-0.113684</td>\n",
              "      <td>0.010517</td>\n",
              "      <td>-0.054826</td>\n",
              "      <td>-0.017351</td>\n",
              "      <td>0.006764</td>\n",
              "      <td>-0.093621</td>\n",
              "      <td>0.028507</td>\n",
              "      <td>-0.006202</td>\n",
              "      <td>-0.027301</td>\n",
              "      <td>...</td>\n",
              "      <td>0.035365</td>\n",
              "      <td>0.002757</td>\n",
              "      <td>0.069563</td>\n",
              "      <td>-0.060976</td>\n",
              "      <td>0.032011</td>\n",
              "      <td>-0.086696</td>\n",
              "      <td>0.083038</td>\n",
              "      <td>-0.045452</td>\n",
              "      <td>-0.004372</td>\n",
              "      <td>-0.046102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.037118</td>\n",
              "      <td>-0.103141</td>\n",
              "      <td>-0.009685</td>\n",
              "      <td>-0.055974</td>\n",
              "      <td>-0.036016</td>\n",
              "      <td>0.019632</td>\n",
              "      <td>-0.090301</td>\n",
              "      <td>0.024423</td>\n",
              "      <td>-0.009795</td>\n",
              "      <td>-0.025391</td>\n",
              "      <td>...</td>\n",
              "      <td>0.031072</td>\n",
              "      <td>-0.008762</td>\n",
              "      <td>0.066580</td>\n",
              "      <td>-0.053213</td>\n",
              "      <td>0.017040</td>\n",
              "      <td>-0.086368</td>\n",
              "      <td>0.088988</td>\n",
              "      <td>-0.035956</td>\n",
              "      <td>-0.012113</td>\n",
              "      <td>-0.037199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.011538</td>\n",
              "      <td>-0.056413</td>\n",
              "      <td>0.009323</td>\n",
              "      <td>-0.023996</td>\n",
              "      <td>-0.008244</td>\n",
              "      <td>0.010347</td>\n",
              "      <td>-0.049965</td>\n",
              "      <td>0.019880</td>\n",
              "      <td>0.005599</td>\n",
              "      <td>-0.014451</td>\n",
              "      <td>...</td>\n",
              "      <td>0.020434</td>\n",
              "      <td>-0.009908</td>\n",
              "      <td>0.025510</td>\n",
              "      <td>-0.040189</td>\n",
              "      <td>0.020598</td>\n",
              "      <td>-0.039409</td>\n",
              "      <td>0.043822</td>\n",
              "      <td>-0.028550</td>\n",
              "      <td>-0.011699</td>\n",
              "      <td>-0.031750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.012399</td>\n",
              "      <td>-0.043797</td>\n",
              "      <td>0.003580</td>\n",
              "      <td>-0.009165</td>\n",
              "      <td>-0.005519</td>\n",
              "      <td>0.002945</td>\n",
              "      <td>-0.031241</td>\n",
              "      <td>0.013018</td>\n",
              "      <td>0.007798</td>\n",
              "      <td>-0.017230</td>\n",
              "      <td>...</td>\n",
              "      <td>0.017786</td>\n",
              "      <td>-0.010820</td>\n",
              "      <td>0.019272</td>\n",
              "      <td>-0.018581</td>\n",
              "      <td>0.012299</td>\n",
              "      <td>-0.044582</td>\n",
              "      <td>0.029489</td>\n",
              "      <td>-0.016724</td>\n",
              "      <td>-0.011484</td>\n",
              "      <td>-0.026900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.013598</td>\n",
              "      <td>-0.056186</td>\n",
              "      <td>0.002418</td>\n",
              "      <td>-0.012760</td>\n",
              "      <td>-0.011633</td>\n",
              "      <td>0.008843</td>\n",
              "      <td>-0.045815</td>\n",
              "      <td>0.007909</td>\n",
              "      <td>0.005343</td>\n",
              "      <td>-0.012376</td>\n",
              "      <td>...</td>\n",
              "      <td>0.025791</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.034695</td>\n",
              "      <td>-0.026503</td>\n",
              "      <td>0.016453</td>\n",
              "      <td>-0.034243</td>\n",
              "      <td>0.033142</td>\n",
              "      <td>-0.031705</td>\n",
              "      <td>-0.004628</td>\n",
              "      <td>-0.023777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.013443</td>\n",
              "      <td>-0.030104</td>\n",
              "      <td>0.002643</td>\n",
              "      <td>-0.014531</td>\n",
              "      <td>-0.006054</td>\n",
              "      <td>0.004171</td>\n",
              "      <td>-0.016316</td>\n",
              "      <td>-0.002922</td>\n",
              "      <td>-0.008916</td>\n",
              "      <td>-0.012167</td>\n",
              "      <td>...</td>\n",
              "      <td>0.015958</td>\n",
              "      <td>0.000178</td>\n",
              "      <td>0.014265</td>\n",
              "      <td>-0.010907</td>\n",
              "      <td>-0.001317</td>\n",
              "      <td>-0.014783</td>\n",
              "      <td>0.014164</td>\n",
              "      <td>-0.015839</td>\n",
              "      <td>-0.008469</td>\n",
              "      <td>-0.008833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.011874</td>\n",
              "      <td>-0.060857</td>\n",
              "      <td>0.002392</td>\n",
              "      <td>-0.033200</td>\n",
              "      <td>-0.001622</td>\n",
              "      <td>-0.003583</td>\n",
              "      <td>-0.036732</td>\n",
              "      <td>0.018217</td>\n",
              "      <td>-0.000708</td>\n",
              "      <td>-0.028152</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013472</td>\n",
              "      <td>-0.004408</td>\n",
              "      <td>0.032136</td>\n",
              "      <td>-0.038755</td>\n",
              "      <td>0.004184</td>\n",
              "      <td>-0.055207</td>\n",
              "      <td>0.052054</td>\n",
              "      <td>-0.034740</td>\n",
              "      <td>-0.000163</td>\n",
              "      <td>-0.020475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0.025350</td>\n",
              "      <td>-0.062244</td>\n",
              "      <td>-0.000864</td>\n",
              "      <td>-0.018462</td>\n",
              "      <td>-0.012985</td>\n",
              "      <td>0.007511</td>\n",
              "      <td>-0.050356</td>\n",
              "      <td>0.011866</td>\n",
              "      <td>-0.001923</td>\n",
              "      <td>-0.011826</td>\n",
              "      <td>...</td>\n",
              "      <td>0.025004</td>\n",
              "      <td>-0.006105</td>\n",
              "      <td>0.041934</td>\n",
              "      <td>-0.027537</td>\n",
              "      <td>0.005924</td>\n",
              "      <td>-0.048499</td>\n",
              "      <td>0.042107</td>\n",
              "      <td>-0.031417</td>\n",
              "      <td>-0.003861</td>\n",
              "      <td>-0.021519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.039010</td>\n",
              "      <td>-0.079311</td>\n",
              "      <td>0.010553</td>\n",
              "      <td>-0.038510</td>\n",
              "      <td>-0.016680</td>\n",
              "      <td>-0.003245</td>\n",
              "      <td>-0.064450</td>\n",
              "      <td>0.032365</td>\n",
              "      <td>0.002552</td>\n",
              "      <td>-0.026993</td>\n",
              "      <td>...</td>\n",
              "      <td>0.037364</td>\n",
              "      <td>0.004201</td>\n",
              "      <td>0.057296</td>\n",
              "      <td>-0.041734</td>\n",
              "      <td>0.030813</td>\n",
              "      <td>-0.079211</td>\n",
              "      <td>0.063384</td>\n",
              "      <td>-0.039762</td>\n",
              "      <td>-0.016524</td>\n",
              "      <td>-0.045097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.032695</td>\n",
              "      <td>-0.093455</td>\n",
              "      <td>0.003851</td>\n",
              "      <td>-0.029507</td>\n",
              "      <td>-0.024222</td>\n",
              "      <td>0.004183</td>\n",
              "      <td>-0.082628</td>\n",
              "      <td>0.023965</td>\n",
              "      <td>0.003864</td>\n",
              "      <td>-0.031732</td>\n",
              "      <td>...</td>\n",
              "      <td>0.032263</td>\n",
              "      <td>0.001557</td>\n",
              "      <td>0.048925</td>\n",
              "      <td>-0.046006</td>\n",
              "      <td>0.024169</td>\n",
              "      <td>-0.075976</td>\n",
              "      <td>0.076879</td>\n",
              "      <td>-0.034149</td>\n",
              "      <td>-0.010713</td>\n",
              "      <td>-0.044931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0.004442</td>\n",
              "      <td>-0.026212</td>\n",
              "      <td>0.009902</td>\n",
              "      <td>-0.001822</td>\n",
              "      <td>-0.008828</td>\n",
              "      <td>0.010916</td>\n",
              "      <td>-0.030454</td>\n",
              "      <td>0.002926</td>\n",
              "      <td>-0.005082</td>\n",
              "      <td>-0.013701</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005688</td>\n",
              "      <td>-0.005324</td>\n",
              "      <td>0.017292</td>\n",
              "      <td>-0.017578</td>\n",
              "      <td>0.004316</td>\n",
              "      <td>-0.021007</td>\n",
              "      <td>0.029398</td>\n",
              "      <td>-0.005751</td>\n",
              "      <td>0.000082</td>\n",
              "      <td>-0.016709</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>49 rows  50 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2         3         4         5         6   \\\n",
              "0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "1   0.152393 -0.416267  0.030145 -0.184082 -0.010394  0.014373 -0.381159   \n",
              "2   0.159556 -0.416011  0.045277 -0.178962 -0.038984  0.019003 -0.383190   \n",
              "3   0.165171 -0.473175  0.043206 -0.210630 -0.084367  0.019195 -0.367736   \n",
              "4   0.116846 -0.401664  0.018386 -0.184641 -0.030639  0.003306 -0.373258   \n",
              "5   0.141465 -0.384300  0.016896 -0.162110 -0.051683  0.004966 -0.369213   \n",
              "6   0.166352 -0.469436  0.008980 -0.197826 -0.099537  0.031281 -0.373684   \n",
              "7   0.149645 -0.411599  0.010558 -0.186049 -0.039491  0.012580 -0.374992   \n",
              "8   0.140083 -0.413539  0.013998 -0.192300 -0.058990  0.018132 -0.367364   \n",
              "9   0.154283 -0.358461  0.027813 -0.162773 -0.021588  0.004086 -0.379868   \n",
              "10  0.161227 -0.442374  0.014481 -0.199751 -0.089896  0.034551 -0.346453   \n",
              "11  0.155886 -0.447800  0.018727 -0.185921 -0.073807  0.018182 -0.368839   \n",
              "12  0.162361 -0.455236  0.052984 -0.212550 -0.099726  0.018839 -0.332478   \n",
              "13  0.143696 -0.427579  0.014258 -0.193974 -0.058986  0.019184 -0.354232   \n",
              "14  0.145564 -0.404659  0.012986 -0.178703 -0.071463  0.010354 -0.335715   \n",
              "15  0.168089 -0.457519  0.013693 -0.196008 -0.086993  0.023595 -0.365521   \n",
              "16  0.169317 -0.424889  0.028299 -0.191357 -0.067590  0.025758 -0.337540   \n",
              "17  0.161842 -0.418257  0.007604 -0.175646 -0.058127  0.031463 -0.340421   \n",
              "18  0.151130 -0.409978 -0.001362 -0.177443 -0.056593  0.019374 -0.337033   \n",
              "19  0.140722 -0.410099 -0.006606 -0.186926 -0.057433  0.009434 -0.347329   \n",
              "20  0.129275 -0.371645  0.001673 -0.171600 -0.063296  0.023654 -0.326359   \n",
              "21  0.178906 -0.447389  0.018909 -0.204372 -0.077419  0.027247 -0.363948   \n",
              "22  0.136288 -0.395735  0.020039 -0.181087 -0.075776  0.014532 -0.321146   \n",
              "23  0.164449 -0.452591  0.008049 -0.198805 -0.076200  0.022188 -0.387635   \n",
              "24  0.178088 -0.463304  0.005490 -0.215836 -0.082049  0.022671 -0.400235   \n",
              "25  0.362555 -0.718820 -0.102157 -0.309125 -0.318396  0.144660 -0.558377   \n",
              "26  0.146703 -0.387724 -0.000509 -0.178179 -0.065141  0.035494 -0.306295   \n",
              "27  0.297862 -0.697382 -0.025332 -0.314447 -0.204552  0.075596 -0.555558   \n",
              "28  0.154398 -0.415433  0.010304 -0.184295 -0.076809  0.026254 -0.354758   \n",
              "29  0.299048 -0.703128 -0.005496 -0.307518 -0.168284  0.071806 -0.559675   \n",
              "30  0.161930 -0.404691  0.000041 -0.184292 -0.087165  0.042113 -0.339819   \n",
              "31  0.262786 -0.660672  0.014405 -0.295140 -0.150632  0.046025 -0.533821   \n",
              "32  0.270357 -0.620131 -0.013387 -0.269225 -0.165449  0.065782 -0.520012   \n",
              "33  0.240564 -0.578682  0.004710 -0.253639 -0.144323  0.050925 -0.473558   \n",
              "34  0.333930 -0.801938  0.019306 -0.343139 -0.189044  0.064471 -0.651507   \n",
              "35  0.220599 -0.524615  0.002625 -0.234627 -0.115668  0.037460 -0.439696   \n",
              "36  0.162020 -0.403671 -0.004301 -0.182414 -0.084728  0.035025 -0.336245   \n",
              "37  0.161339 -0.417299  0.007067 -0.189233 -0.087707  0.036001 -0.345543   \n",
              "38  0.040550 -0.113684  0.010517 -0.054826 -0.017351  0.006764 -0.093621   \n",
              "39  0.037118 -0.103141 -0.009685 -0.055974 -0.036016  0.019632 -0.090301   \n",
              "40  0.011538 -0.056413  0.009323 -0.023996 -0.008244  0.010347 -0.049965   \n",
              "41  0.012399 -0.043797  0.003580 -0.009165 -0.005519  0.002945 -0.031241   \n",
              "42  0.013598 -0.056186  0.002418 -0.012760 -0.011633  0.008843 -0.045815   \n",
              "43  0.013443 -0.030104  0.002643 -0.014531 -0.006054  0.004171 -0.016316   \n",
              "44  0.011874 -0.060857  0.002392 -0.033200 -0.001622 -0.003583 -0.036732   \n",
              "45  0.025350 -0.062244 -0.000864 -0.018462 -0.012985  0.007511 -0.050356   \n",
              "46  0.039010 -0.079311  0.010553 -0.038510 -0.016680 -0.003245 -0.064450   \n",
              "47  0.032695 -0.093455  0.003851 -0.029507 -0.024222  0.004183 -0.082628   \n",
              "48  0.004442 -0.026212  0.009902 -0.001822 -0.008828  0.010916 -0.030454   \n",
              "\n",
              "          7         8         9     ...           40        41        42  \\\n",
              "0   0.000000  0.000000  0.000000    ...     0.000000  0.000000  0.000000   \n",
              "1   0.049737 -0.049526 -0.164221    ...     0.166599 -0.032823  0.178357   \n",
              "2   0.090034 -0.052516 -0.163107    ...     0.164741 -0.029205  0.210783   \n",
              "3   0.125834 -0.021669 -0.154705    ...     0.160588 -0.022733  0.267268   \n",
              "4   0.049373 -0.072604 -0.159970    ...     0.176916 -0.018187  0.153957   \n",
              "5   0.071801 -0.044377 -0.149465    ...     0.156825 -0.028656  0.198274   \n",
              "6   0.109303 -0.043478 -0.165713    ...     0.153302 -0.031705  0.270461   \n",
              "7   0.079210 -0.049920 -0.154188    ...     0.150584 -0.023231  0.220580   \n",
              "8   0.084785 -0.042703 -0.153140    ...     0.143403 -0.028404  0.219374   \n",
              "9   0.057855 -0.037082 -0.157236    ...     0.129902 -0.025992  0.171931   \n",
              "10  0.123110 -0.027605 -0.152852    ...     0.137941 -0.030926  0.261681   \n",
              "11  0.105111 -0.028799 -0.155093    ...     0.145346 -0.041099  0.232017   \n",
              "12  0.139945 -0.005306 -0.179349    ...     0.159146 -0.026906  0.261907   \n",
              "13  0.097019 -0.044185 -0.166540    ...     0.163962 -0.037522  0.236092   \n",
              "14  0.102695 -0.031890 -0.158180    ...     0.136381 -0.033189  0.225300   \n",
              "15  0.106462 -0.042469 -0.173147    ...     0.155501 -0.035328  0.257853   \n",
              "16  0.098285 -0.020237 -0.150555    ...     0.150408 -0.030167  0.221441   \n",
              "17  0.098849 -0.039084 -0.142567    ...     0.133845 -0.019111  0.233122   \n",
              "18  0.083588 -0.039911 -0.134523    ...     0.156037 -0.023175  0.231816   \n",
              "19  0.084337 -0.035972 -0.141067    ...     0.139103 -0.039580  0.219179   \n",
              "20  0.089162 -0.043492 -0.131944    ...     0.149934 -0.025618  0.203660   \n",
              "21  0.112033 -0.030628 -0.162722    ...     0.154639 -0.036913  0.257221   \n",
              "22  0.093421 -0.038821 -0.147707    ...     0.143629 -0.013621  0.198745   \n",
              "23  0.116562 -0.015153 -0.150241    ...     0.164896 -0.020709  0.256107   \n",
              "24  0.124827 -0.020150 -0.160930    ...     0.152545 -0.020881  0.274300   \n",
              "25  0.332495  0.034540 -0.147916    ...     0.239089  0.043642  0.643740   \n",
              "26  0.096835 -0.029075 -0.125234    ...     0.134478 -0.013227  0.231356   \n",
              "27  0.256317  0.010092 -0.197969    ...     0.232850 -0.008428  0.484415   \n",
              "28  0.099068 -0.021618 -0.146321    ...     0.137758 -0.026532  0.245911   \n",
              "29  0.214469 -0.002554 -0.214621    ...     0.231295 -0.027927  0.446648   \n",
              "30  0.111491 -0.017846 -0.130760    ...     0.136056 -0.014352  0.256241   \n",
              "31  0.197730 -0.011116 -0.218248    ...     0.218821 -0.029363  0.402148   \n",
              "32  0.206397 -0.002897 -0.192744    ...     0.205340 -0.015874  0.416355   \n",
              "33  0.179806 -0.005000 -0.181215    ...     0.191583 -0.017583  0.359153   \n",
              "34  0.239165 -0.031385 -0.267168    ...     0.268294 -0.029644  0.500985   \n",
              "35  0.152936 -0.014268 -0.167562    ...     0.174674 -0.018453  0.319271   \n",
              "36  0.116388 -0.007195 -0.141909    ...     0.127347 -0.020683  0.257507   \n",
              "37  0.121980 -0.013940 -0.143837    ...     0.140387 -0.028504  0.262242   \n",
              "38  0.028507 -0.006202 -0.027301    ...     0.035365  0.002757  0.069563   \n",
              "39  0.024423 -0.009795 -0.025391    ...     0.031072 -0.008762  0.066580   \n",
              "40  0.019880  0.005599 -0.014451    ...     0.020434 -0.009908  0.025510   \n",
              "41  0.013018  0.007798 -0.017230    ...     0.017786 -0.010820  0.019272   \n",
              "42  0.007909  0.005343 -0.012376    ...     0.025791  0.000015  0.034695   \n",
              "43 -0.002922 -0.008916 -0.012167    ...     0.015958  0.000178  0.014265   \n",
              "44  0.018217 -0.000708 -0.028152    ...     0.013472 -0.004408  0.032136   \n",
              "45  0.011866 -0.001923 -0.011826    ...     0.025004 -0.006105  0.041934   \n",
              "46  0.032365  0.002552 -0.026993    ...     0.037364  0.004201  0.057296   \n",
              "47  0.023965  0.003864 -0.031732    ...     0.032263  0.001557  0.048925   \n",
              "48  0.002926 -0.005082 -0.013701    ...     0.005688 -0.005324  0.017292   \n",
              "\n",
              "          43        44        45        46        47        48        49  \n",
              "0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
              "1  -0.335843  0.146869 -0.470962  0.395400 -0.283806  0.012441 -0.268792  \n",
              "2  -0.320233  0.155152 -0.453669  0.422672 -0.289768 -0.012544 -0.287482  \n",
              "3  -0.264041  0.155710 -0.411877  0.391534 -0.261104 -0.058292 -0.228538  \n",
              "4  -0.338802  0.137904 -0.475165  0.399816 -0.319382  0.042911 -0.280524  \n",
              "5  -0.304889  0.146665 -0.450109  0.394797 -0.290799 -0.008019 -0.269526  \n",
              "6  -0.290961  0.125270 -0.424231  0.403370 -0.244946 -0.033692 -0.220371  \n",
              "7  -0.321817  0.139477 -0.430551  0.406725 -0.280014  0.007822 -0.259643  \n",
              "8  -0.318501  0.148712 -0.445012  0.404945 -0.276043 -0.022083 -0.253724  \n",
              "9  -0.332101  0.152847 -0.401829  0.405083 -0.291005 -0.011911 -0.250736  \n",
              "10 -0.257220  0.110564 -0.374768  0.370192 -0.204983 -0.050538 -0.204457  \n",
              "11 -0.299460  0.148102 -0.425937  0.400460 -0.256242 -0.026433 -0.248019  \n",
              "12 -0.228763  0.123500 -0.343418  0.365565 -0.178925 -0.054551 -0.190233  \n",
              "13 -0.288357  0.128580 -0.407287  0.369781 -0.233996 -0.007775 -0.241931  \n",
              "14 -0.259739  0.122836 -0.388248  0.365997 -0.225620 -0.019534 -0.215720  \n",
              "15 -0.284918  0.117309 -0.406414  0.393192 -0.230904 -0.051237 -0.221984  \n",
              "16 -0.240097  0.123303 -0.352163  0.344231 -0.208145 -0.038033 -0.203953  \n",
              "17 -0.264521  0.102578 -0.375068  0.357668 -0.205485 -0.025895 -0.204281  \n",
              "18 -0.275182  0.110859 -0.413552  0.356258 -0.242073 -0.017939 -0.216695  \n",
              "19 -0.265479  0.105687 -0.380631  0.337854 -0.223201 -0.020439 -0.199714  \n",
              "20 -0.272245  0.124562 -0.400280  0.370291 -0.238899 -0.004114 -0.228013  \n",
              "21 -0.265650  0.120205 -0.379246  0.376951 -0.212463 -0.037797 -0.212692  \n",
              "22 -0.251379  0.116586 -0.379648  0.348729 -0.225348 -0.004521 -0.212112  \n",
              "23 -0.277784  0.125921 -0.403735  0.373134 -0.235530 -0.052409 -0.226917  \n",
              "24 -0.281758  0.129352 -0.417016  0.393995 -0.249052 -0.050764 -0.223596  \n",
              "25 -0.206322  0.013518 -0.510323  0.507142  0.036080 -0.277276 -0.131295  \n",
              "26 -0.223501  0.101697 -0.340727  0.311853 -0.189669 -0.040087 -0.190094  \n",
              "27 -0.314565  0.103996 -0.544687  0.531808 -0.182946 -0.161261 -0.225704  \n",
              "28 -0.249538  0.111585 -0.382625  0.356162 -0.223341 -0.039649 -0.191929  \n",
              "29 -0.348223  0.140533 -0.551988  0.544470 -0.253276 -0.109701 -0.272835  \n",
              "30 -0.223289  0.110703 -0.349291  0.342871 -0.200392 -0.058272 -0.191546  \n",
              "31 -0.358907  0.140617 -0.536718  0.520189 -0.272818 -0.101143 -0.275408  \n",
              "32 -0.326325  0.120232 -0.507643  0.492229 -0.226345 -0.120845 -0.250378  \n",
              "33 -0.308230  0.121115 -0.475700  0.463977 -0.220983 -0.092057 -0.247100  \n",
              "34 -0.436792  0.197332 -0.675428  0.659791 -0.359941 -0.121163 -0.351272  \n",
              "35 -0.280031  0.128991 -0.432398  0.419654 -0.222269 -0.068565 -0.228950  \n",
              "36 -0.212490  0.087714 -0.334785  0.328358 -0.171171 -0.061216 -0.168949  \n",
              "37 -0.222087  0.091713 -0.345795  0.331689 -0.181318 -0.058511 -0.182921  \n",
              "38 -0.060976  0.032011 -0.086696  0.083038 -0.045452 -0.004372 -0.046102  \n",
              "39 -0.053213  0.017040 -0.086368  0.088988 -0.035956 -0.012113 -0.037199  \n",
              "40 -0.040189  0.020598 -0.039409  0.043822 -0.028550 -0.011699 -0.031750  \n",
              "41 -0.018581  0.012299 -0.044582  0.029489 -0.016724 -0.011484 -0.026900  \n",
              "42 -0.026503  0.016453 -0.034243  0.033142 -0.031705 -0.004628 -0.023777  \n",
              "43 -0.010907 -0.001317 -0.014783  0.014164 -0.015839 -0.008469 -0.008833  \n",
              "44 -0.038755  0.004184 -0.055207  0.052054 -0.034740 -0.000163 -0.020475  \n",
              "45 -0.027537  0.005924 -0.048499  0.042107 -0.031417 -0.003861 -0.021519  \n",
              "46 -0.041734  0.030813 -0.079211  0.063384 -0.039762 -0.016524 -0.045097  \n",
              "47 -0.046006  0.024169 -0.075976  0.076879 -0.034149 -0.010713 -0.044931  \n",
              "48 -0.017578  0.004316 -0.021007  0.029398 -0.005751  0.000082 -0.016709  \n",
              "\n",
              "[49 rows x 50 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "F8Cc4Y5sss_p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Testing**"
      ]
    },
    {
      "metadata": {
        "id": "7JW5z3-ZUIc3",
        "colab_type": "code",
        "outputId": "06297e33-134f-4a77-f586-4c8f4fad6499",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from keras.models import load_model\n",
        "\n",
        "model = load_model(best_model_path)\n",
        "\n",
        "test_sentence_pairs = [('What can make Physics easy to learn?','How can you make physics easy to learn?'),('How many times a day do a clocks hands overlap?','What does it mean that every time I look at the clock the numbers are the same?')]\n",
        "\n",
        "test_data_x1, test_data_x2, leaks_test = create_test_data(tokenizer,test_sentence_pairs,  siamese_config['MAX_SEQUENCE_LENGTH'])\n",
        "\n",
        "preds = list(model.predict([test_data_x1, test_data_x2, leaks_test], verbose=1).ravel())\n",
        "results = [(x, y, z) for (x, y), z in zip(test_sentence_pairs, preds)]\n",
        "results.sort(key=itemgetter(2), reverse=True)\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r2/2 [==============================] - 1s 539ms/step\n",
            "[('What can make Physics easy to learn?', 'How can you make physics easy to learn?', 0.48317868), ('How many times a day do a clocks hands overlap?', 'What does it mean that every time I look at the clock the numbers are the same?', 0.44244337)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3bcMO4gDsyvO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}